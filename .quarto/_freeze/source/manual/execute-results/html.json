{
  "hash": "eaebc78c4bc3aa6409f42d8af71f19f3",
  "result": {
    "markdown": "---\ntitle: \"Getting started\"\n\nexecute:\n  eval: false\n\nengine: knitr\n---\n\n\n\n\n## Prerequisites\n\nTo run the workflow conda/mamba and snakemake are required. \n\n- To install snakemake, check out the installation instructions found [here](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html). The workflow was so far tested with snakemake version 7.32.4.\n- Information about installing conda can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) and for mamba [here]()[https://mamba.readthedocs.io/en/latest/mamba-installation.html].\n\n\n## Installation\n\nFungomics can be installed via \n\n\n::: {.cell}\n\n```{.bash .cell-code}\ngit clone xx.git\n```\n:::\n\n\n\n\n## Run NanoITS\n\n### General setup\n\nTo run NanoITS you need to provide some information about the samples you want to analyse via the config.yaml file. \nStart with moving the `config/config.yaml` file to the folder with the fastq files you want to analyse and open the file with `nano`.\n\nThere are several things you can modify:\n\n1. Provide the project name in `project: \"run_v1\"`. Your results will be generated in the folder you start the snakemake workflow in and the results will be generated in `results/<project_name>`. Your project name can contain letters, numbers `_` and `-`. Please do not use other symbols, such as spaces.\n2. Provide a mapping file that describes the samples you want to analyse, i.e. `samples_file: \"input/mapping.csv\"`. Here:\n   1. sample: The names of your sample. This id will be used to label all files created in subsequent steps. In your sample names you can loose letters, numbers and `-`. Please do not use other symbols spaces, dots or underscores in your sample names. \n   2. barcode: The barcode ID. Can be empty as it is not actively used in the workflow\n   3. path: Path to the fastq.gz files. The workflow accepts one file per barcode, so if you have more than one file merge this first using cat. Here, you can use a relative path (i.e. relative to the working directory you start the snakemake workflow in) or absolute. \n3. Decide what classifiers you want to use in `classifiers: [\"minimap2\", \"kraken2\"]`. Currently, two classifiers are implemented: (a) the alignment-based classifier minimap2 and (b) the kmer-based classifier kraken2. You can use both or either of the two classifiers.\n4. Decide what markers you want to analyse in `markers: [\"SSU\", \"ITS1\"]`. The workflow was developed for primers targetting both the SSU and ITS1 but the workflow will also run for either option selected and we plan to in the future extend the workflow to also accept the LSU rRNA gene.\n5. Change tool specific parameters: If desires, there are several parameters that can be changed by the users, such as the numbers of threads to use, the settings for the read filtering or the classification.\n\n\nExample mapping file:\n\n```\nsample,barcode,path\nbc01,barcode01,/path/barcode01.fastq.gz\nbc02,barcode02,/path/barcode02.fastq.gz\n...\n```\n\nSeveral steps of this workflow are quite heavy, so we recommend running this worklow on an HPC. If you want, you can of course try running it on a desktop computer.\n\n### Test-run\n\nTo test whether the workflow will run successfully:\n\n1. Provide the path to where you installed NanoITS afer --s\n2. Provide the path to the config file after --configfile\n3. Provide the path to where you want snakemake to install all program dependencies. We recommend to install these into the folder in which you downloaded NanoITS but you can change this if desired\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#activate conda environment with your snakemake installation, i.e. \nmamba activate snakemake_7.32.4\n\n#test if everything runs as extended \nsnakemake --use-conda --cores 1 -s <path_to_NanoITS_install>/workflow/Snakefile --configfile config/config.yaml --conda-prefix <path_to_NanoITS_install>/workflow/.snakemake/conda  --rerun-incomplete --nolock -np \n```\n:::\n\n\nIf the test-run was successful you can run snakemake interactively with:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsnakemake --use-conda --cores 1 -s <path_to_NanoITS_install>/workflow/Snakefile --configfile config/config.yaml --conda-prefix <path_to_NanoITS_install>/workflow/.snakemake/conda  --rerun-incomplete --nolock\n```\n:::\n\n\nOr via a slurm HPC, such as UvAÂ´s crunchomics server by doing the following edits in jobscript.sh:\n\n1. Exchange the line that utilizes snakemake with the line of code that you used for the testrun to ensure that all paths are set correctly.\n2. Adjust the number of CPUs as needed\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsbatch jobscript.sh\n```\n:::\n\n\n\n\n\n### Development stage\n\n#### Setup snakemake\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmamba create -n snakemake_7.32.4 -c conda-forge -c bioconda snakemake=7.32.4\n```\n:::\n\n::: {.cell}\n\n```{.bash .cell-code}\nconda deactivate\nexport LC_ALL=en_US.UTF-8\n\n#set working directory\nwdir=\"/home/ndombro/personal/snakemake_workflows/NanoITS\"\n\ncd $wdir \n\nmamba activate snakemake_7.32.4\n```\n:::\n\n\n\n#### Organize folder structure\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#organize folders\n# mkdir -p workflow/scripts\n# mkdir workflow/envs\n# mkdir workflow/rules\n# mkdir workflow/report\n\nmkdir docs\nmkdir config\n\nmkdir -p input/raw\nmkdir -p input/combined\n\necho -e 'barcode02\\nbarcode05' > input/samples.txt\necho -e \"test\" >  workflow/report/workflow.rst\n\ncp /home/ndombro/personal/snakemake_workflows/fungomics/config/config.yaml config\ncp ~/personal/snakemake_workflows/fungomics/jobscript.sh .\n```\n:::\n\n\n\n\n#### Get yaml for existing workflows\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nconda activate nanopore\nconda env export > workflow/envs/nanopore.yaml\nconda deactivate\n\nconda activate r_for_amplicon\nconda env export > workflow/envs/r_for_amplicon.yaml \nconda deactivate\n```\n:::\n\n\n\n#### Prepare db folder\n\nWe will prepare a db folder that can be downloaded from zenodo. The reason for doing that is because unite can not be downloaded via wget since one needs to fill out a form. Therefore, we will the already prepared databases via zenodo.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncd /home/ndombro/personal/for_zenodo/fungomics/db\n\nmkdir unite/general\nmkdir silva/general\n\n#db used by minimap\ncp ~/personal/projects/fungomics/mangrove/db/unite/general_release/unite-ref-seqs.fna unite/general/\ncp ~/personal/projects/fungomics/mangrove/db/unite/general_release/unite-ref-taxonomy.txt unite/general/\n\ncp ~/personal/projects/fungomics/mangrove/db/silva/general/silva-ref-taxonomy.txt silva/general/\ncp ~/personal/projects/fungomics/mangrove/db/silva/general/silva-ref.fasta silva/general/\n\n#db used by kraken\ncp -r ~/personal/projects/fungomics/mangrove/db/unite/general_release/db_kraken/* unite/\n\ncp -r ~/personal/projects/fungomics/mangrove/db/silva/kraken silva/\n\n#clean up symbolic links\ncd silva/kraken\nrm -r taxonomy\ncp -r /home/ndombro/personal/projects/fungomics/mangrove/db/ncbi/taxonomy/ .\n\ncd ../../..\n\n#compress\ntar -zcvf db.tar.gz db\n```\n:::\n\n\n\n\n#### Prepare test data\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nln -s /zfs/omics/personal/ndombro/projects/fungomics/mangrove/data/fastq_pass/barcode02/ ${PWD}/input/raw\nln -s /zfs/omics/personal/ndombro/projects/fungomics/mangrove/data/fastq_pass/barcode05/ ${PWD}/input/raw\n\nfor i in `cat input/samples.txt`; do\n  cat input/raw/${i}/*fastq.gz > input/combined/${i}.fastq.gz\ndone\n\n# cp input/raw/barcode02/aps797_pass_barcode02_72052f83_778f88e7_2.fastq.gz input/combined/barcode02.fastq.gz\n# cp input/raw/barcode05/aps797_pass_barcode05_72052f83_778f88e7_2.fastq.gz input/combined/barcode05.fastq.gz\n\necho -e 'sample,barcode,path\\nbc02,barcode02,input/combined/barcode02.fastq.gz\\nbc05,barcode05,input/combined/barcode05.fastq.gz' > input/mapping.txt\n\necho $(zcat input/combined/barcode02.fastq.gz | wc -l)/4|bc\necho $(zcat input/combined/barcode05.fastq.gz | wc -l)/4|bc\n\necho $(zcat input/raw/barcode02/*fastq.gz | wc -l)/4|bc\n```\n:::\n\n::: {.cell}\n\n```{.bash .cell-code}\n#dry-run\nsnakemake --use-conda --cores 1 --configfile config/config.yaml --conda-prefix workflow/.snakemake/conda  --rerun-incomplete --nolock -np \n\n#full run\nsbatch jobscript.sh\n\n#generate report\nsnakemake --report report.html --configfile config/config.yaml \n\nRscript {my_basedir}/scripts/otu_analysis.R results/{project}/classification/*/${{i}}.merged.outmat.tsv ${{i}} results/{project}\n\n\nfile_paths <- c(\"results/test_run/classification/minimap2/SSU.merged.outmat.tsv\", \"results/test_run/classification/kraken2/SSU.merged.outmat.tsv\")\nmarker_id <- \"SSU\"\n\n\n\nresults/test_run/classification/minimap2/SSU.merged.outmat.tsv\nresults/test_run/classification/kraken2/SSU.merged.outmat.tsv\n```\n:::\n\n\n\n\n## Other notes\n\n\n### Comment on fickle script:\n\nIf you adjust something in the file path then `workflow/scripts/tomat.py` needs to be adjusted, esp this part:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npathfile=\" results/test_run/classification/minimap2/bc02/bc02_ITS1_minimap2.taxlist\"\n\nparts = pathfile.split('/')\ndir = '/'.join(parts[:4])\n\nproject = parts[1]\nmethod = parts[3]\nsample = parts[4]\nmarker = parts[5].split('_')[1]\n\noutpath = f\"{dir}/{sample}/{sample}_{marker}_{method}\"\n```\n:::\n\n\n\nIn `workflow/scripts/merge_otu_tables.py` adjust:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbarcode = os.path.join(root, dir).split(os.path.sep)[4]\n```\n:::\n\n\n\n\n\n### Notes on how the taxonomy files were generated\n\n\nCurrently, the workflow supports two databases:\n\n- Unite for ITS1 marker gene analyses (linked to the Unite taxonomy)\n- Silva for 16S/18S rRNA gene analyes (linked to the NCBI taxonomy)\n\nBelow, you find the code to:\n\n1. Download the sequences for both databases\n2. Download a taxonomy mapping file (Silva) or generate a custom mapping file (Unite)\n3. Parse these two to be suited for use in the Minimap2 and Kraken2 classification step\n\n\n### Unite (ITS)\n\n#### Download db\n\nUnite can not be downloaded via wget since one needs to receive a download link via mail. But the data was downloaded via the web browser on 0210223 from [this website](https://doi.plutof.ut.ee/doi/10.15156/BIO/2938069) and [this website](https://doi.plutof.ut.ee/doi/10.15156/BIO/2938081) then uploaded to crunchomics into the working directory and this folder `db/unite/general_release`:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmkdir -p db/unite/general_release\n\ncd db/unite/general_release\ntar -zxvf sh_general_release_all_18.07.2023.tgz\ncd ../..\n```\n:::\n\n\n#### Parse db for minimap2\n\nArguments for `shorten_fasta_headers.py`:\n\n- `--header_part INT`: The unite file has a longer header with different elements separated by \"|\", here we just decide which element after separation we want to keep. \n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#shorten header \npython scripts/shorten_fasta_headers.py -i db/unite/general_release/sh_general_release_dynamic_all_18.07.2023.fasta -o db/unite/general_release/unite-ref-seqs.fna --header_part 3\n\n#check if headers are unique\ngrep \">\" db/unite/general_release/unite-ref-seqs.fna | sort | uniq -d\n\n#make a taxon mapping file \npython scripts/parse_unite_tax.py -i db/unite/general_release/sh_general_release_dynamic_all_18.07.2023.fasta  -o db/unite/general_release/unite-ref-taxonomy.txt\n```\n:::\n\n\n\n#### Parse db for kraken2\n\nNotice: To convert the fasta header to a taxonomy mapping file, a perl script that is part of kraken2 was used. This script can be found in the `scripts` folder but originally comes from [here](https://github.com/DerrickWood/kraken2/blob/master/scripts/build_rdp_taxonomy.pl#L30) and was downloaded with `wget -P scripts https://raw.githubusercontent.com/DerrickWood/kraken2/master/scripts/build_rdp_taxonomy.pl`.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nconda activate nanopore\ncd db/unite/general_release\n\nmkdir -p ./db_kraken\n\n#convert fasta header into something that can be used by kraken2 for db generation\nperl -pe '\n  s/>([^\\|]+)\\|([^\\|]+)\\|([^\\|]+)/>$2 $1; $3/;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Fungi/\\tLineage=Root;rootrank;Fungi;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Viridiplantae/\\tLineage=Root;rootrank;Viridiplantae;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Eukaryota_kgd_Incertae_sedis/\\tLineage=Root;rootrank;Eukaryota_kgd_Incertae_sedis;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Alveolata/\\tLineage=Root;rootrank;Alveolata;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Amoebozoa/\\tLineage=Root;rootrank;Amoebozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Apusozoa/\\tLineage=Root;rootrank;Apusozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Choanoflagellozoa/\\tLineage=Root;rootrank;Choanoflagellozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Cryptista/\\tLineage=Root;rootrank;Cryptista;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Euglenozoa/\\tLineage=Root;rootrank;Euglenozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Glaucocystoplantae/\\tLineage=Root;rootrank;Glaucocystoplantae;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Haptista/\\tLineage=Root;rootrank;Haptista;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Heterolobosa/\\tLineage=Root;rootrank;Heterolobosa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Ichthyosporia/\\tLineage=Root;rootrank;Ichthyosporia;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Metazoa/\\tLineage=Root;rootrank;Metazoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Planomonada/\\tLineage=Root;rootrank;Planomonada;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Protista/\\tLineage=Root;rootrank;Protista;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Rhizaria/\\tLineage=Root;rootrank;Rhizaria;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Rhodoplantae/\\tLineage=Root;rootrank;Rhodoplantae;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Stramenopila/\\tLineage=Root;rootrank;Stramenopila;domain/g;\n  s/\\|/ /g;\n  s/;s__*.*//g;\n  s/p__([a-zA-Z0-9_ -]*)/\\1;phylum/g;\n  s/c__([a-zA-Z0-9_ -]*)/\\1;class/g;\n  s/o__([a-zA-Z0-9_ -]*)/\\1;order/g;\n  s/f__([a-zA-Z0-9_ -]*)/\\1;family/g;\n  s/g__([a-zA-Z0-9_ -]*)/\\1;genus/g;\n  s/;$//g'  sh_general_release_dynamic_all_18.07.2023.fasta > ./db_kraken/kunite.fasta\n\n#generate taxonomy mapping files based on the fasta header\nperl ../../../scripts/build_rdp_taxonomy.pl ./db_kraken/kunite.fasta\n\n#organize folder structure\nmkdir -p ./db_kraken/library \nmv ./db_kraken/kunite.fasta ./db_kraken/library/unite.fna\nmkdir -p ./db_kraken/taxonomy\nmv names.dmp nodes.dmp ./db_kraken/taxonomy\nmv seqid2taxid.map ./db_kraken\n\n#buld db\nkraken2-build --build --db db_kraken\n\n#convert to taxID_to_string mapping\ncd db_kraken\n\ntaxonkit lineage --data-dir ./taxonomy <(awk '{print $2}' seqid2taxid.map |\\\n sort | uniq) | \\\n taxonkit reformat -r NA -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir ./taxonomy | \\\n awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' > taxID_to_tax.txt \n\ncd ../../../..\n\nconda deactivate\n```\n:::\n\n\n### Silva (SSU)\n\n#### Download data and parse for general use\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmkdir -p db/silva/general/\nmkdir -p db/ncbi/taxonomy \n\n#download ncbi taxonomy \nwget -N ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz\ntar zxf taxdump.tar.gz -C db/ncbi/taxonomy\nrm taxdump.tar.gz\n\n#download SILVA 16S sequences\ncd db/silva/general \nwget https://www.arb-silva.de/fileadmin/silva_databases/current/Exports/SILVA_138.1_SSURef_NR99_tax_silva.fasta.gz\ngzip -d *.gz\n\n#clean fasta header\ncut -d ' ' -f1 SILVA_138.1_SSURef_NR99_tax_silva.fasta > silva-ref.fasta\n\n#get the silva id to ncbi taxID mapping file (notice: the mapping has a lot of nas, discard)\nwget https://www.arb-silva.de/fileadmin/silva_databases/current/Exports/taxonomy/ncbi/taxmap_embl-ebi_ena_ssu_ref_nr99_138.1.txt.gz -q -O - | gzip -d -c - |  awk '{print $1\".\"$2\".\"$3\"\\t\"$(NF)}' > ref-tax.map\n\ntaxonkit lineage --data-dir db/ncbi/taxonomy <(awk '{print $2}' ref-tax.map | sort | uniq) | \\\n  taxonkit reformat -r NA -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir db/ncbi/taxonomy | \\\n  awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' > taxID_to_tax.txt \n\n#merge the info, for missing hits add a string with NAs\nawk -F'\\t' -v OFS='\\t' 'NR==FNR{taxonomy[$1]=$2; next} {print $1, ($2 in taxonomy) ? taxonomy[$2] : \"D_0__NA;D_1__NA;D_2__NA;D_3__NA;D_4__NA;D_5__NA;D_6__NA\"}' taxID_to_tax.txt  ref-tax.map > silva-ref-taxonomy.txt\n\n#merge things: taxID_to_tax.txt  1 and ref-tax.map 2\nLC_ALL=C join -1 2 -2 1 -t $'\\t' <(LC_ALL=C sort -k 2 ref-tax.map) <(LC_ALL=C sort -k1 taxID_to_tax.txt | sed \"s/ /_/g\") | awk -v OFS=\"\\t\" '{print $2, $3}' > sequence_to_tax.txt\n\ncd ../..\n```\n:::\n\n\n#### Parse and download db for minimap\n\nThe code below is sufficient to read the data into minimap2.\n\n\n#### Parse and download db for kraken\n\nThe code below generates a database to run kraken. Notice, that this does not use the default Silva database with links to the silva taxonomy but generates a database with the SILVA 16S rRNA gene sequences and labels based on the NCBI taxonomy.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nconda activate nanopore \n\nmkdir -p db/silva/kraken\nmkdir -p db/silva/kraken/library \n\n#based on the Silva files generate a mapping file for a fasta header suitable for kraken\npython scripts/generate_header_for_kraken.py -i db/silva/general/sequence_to_tax.txt -o db/silva/general/sequence_to_krakenFmanual.txt\n\n#use mapping file to convert the fasta header into a kraken2 compatible format\npython scripts/convert_fasta_for_kraken.py -f db/silva/general/silva-ref.fasta -m db/silva/general/sequence_to_krakenFmanual.txt -o db/silva/general/silva-ref-modified.fasta\n\n#Add NAs for unassigned taxa and convert to single line fasta with T instead of Us\nsed 's/;;/;NA/g' db/silva/general/silva-ref-modified.fasta | \\\n  sed '/^>/!s/U/T/g'| \\\n  awk '/^>/ {if(NR!=1) printf(\"\\n%s\\n\",$0); else printf(\"%s\\n\",$0); next; } { printf(\"%s\",$0);} END {printf(\"\\n\");}' > db/silva/kraken/library/silva-ref-modified.fna\n\n#add symlink to ncbi taxonomy \nln -s $PWD/db/ncbi/taxonomy $PWD/db/silva/kraken/taxonomy\n\n#generate seqid2taxid.map that links the seqID to the ncbi tax ids\ncp db/silva/general/ref-tax.map db/silva/kraken/seqid2taxid.map\n\n#generate db\n#added fast-build option because run seemed stalled\nsrun -n 1 --cpus-per-task 32 kraken2-build --build --fast-build --threads 32 --db db/silva/kraken  \n\n#generate a mapping file for the sequence ID and tax string\ntaxonkit lineage --data-dir db/silva/kraken_test2/taxonomy <(awk '{print $1}' db/silva/kraken/taxonomy/names.dmp |\\\n sort | uniq) | \\\n taxonkit reformat -r NA -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir db/silva/kraken/taxonomy | \\\n awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' > db/silva/kraken/taxID_to_tax.txt \n\n#reformat a string to be recognized by a downstream script\nsed -i 's|D_0__;D_1__;D_2__;D_3__;D_4__;D_5__;D_6__|D_0__NA;D_1__NA;D_2__NA;D_3__NA;D_4__NA;D_5__NA;D_6__NA|g' db/silva/kraken/taxID_to_tax.txt\n\ncd ../../../..\n\nconda deactivate\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrule download_dbs:\n    output:\n        silva_general_fasta = os.path.join(my_basedir,\"db/silva/general/silva-ref.fasta\"),\n        silva_general_tax = os.path.join(my_basedir, \"db/silva/general/silva-ref-taxonomy.txt\"),\n\n        unite_general_fasta = os.path.join(my_basedir,\"db/unite/general/unite-ref-seqs.fna\"),\n        unite_general_tax = os.path.join(my_basedir, \"db/unite/general/unite-ref-taxonomy.txt\"),\n\n        silva_kraken_db = os.path.join(my_basedir, \"/db/silva/kraken/\"),\n        unite_kraken_db = os.path.join(my_basedir, \"/db/unite/kraken/\")\n    log:\n        \"logs/run_download_db.log\"\n    conda:\n        \"../envs/nanopore.yaml\"\n    threads: 1\n    shell: \"\"\"\n    wget -q -O db.tar.gz https://zenodo.org/records/10052996/files/db.tar.gz?download=1\n    \n    #extract general silva db\n    tar -zxvf db.tar.gz -C {my_basedir}/db/silva/general/ \\\n        --wildcards --no-anchored 'silva-ref.fasta' -O > {output.silva_general_fasta}\n\n    tar -zxvf db.tar.gz -C {my_basedir}/db/silva/general/ \\\n        --wildcards --no-anchored 'silva-ref-taxonomy.txt' -O > {output.silva_general_tax}\n\n    #extract general unite db\n    tar -zxvf db.tar.gz -C {my_basedir}/db/unite/general/ \\\n        --wildcards --no-anchored 'unite-ref-seqs.fna' -O > {output.unite_general_fasta}\n\n    tar -zxvf db.tar.gz -C {my_basedir}/db/unite/general/ \\\n        --wildcards --no-anchored 'unite-ref-taxonomy.txt' -O > {output.unite_general_tax}\n\n    #extract kraken db for silva and unite\n    tar -zxvf db.tar.gz -C {output.silva_kraken_db} --wildcards --no-anchored 'db/silva/kraken/*' --strip-components=3\n    tar -zxvf db.tar.gz -C {output.unite_kraken_db} --wildcards --no-anchored 'db/unite/kraken/*' --strip-components=3\n\n    \"\"\"\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nrule run_minimap_SSU:\n    input:\n        query = \"results/itsx/{sample}/{sample}_{marker}_final.fasta\",\n        silva_general_fasta = os.path.join(my_basedir, \"db/silva/general/silva-ref.fasta\"),\n        unite_general_fasta = os.path.join(my_basedir, \"db/unite/general/unite-ref-seqs.fna\")\n    output:\n        \"results/classification/minimap2/{sample}/{sample}_{marker}_minimap2.paf\"\n    params:\n        threads=config[\"minimap2\"][\"threads\"]\n    resources:\n        mem_mb = lambda wildcards, attempt: attempt * config[\"minimap2\"][\"memory\"]\n    log:\n        \"logs/run_minimap2_{sample}_{marker}.log\"\n    conda:\n        \"../envs/nanopore.yaml\"\n    shell: \"\"\"\n\n    if [ \"{wildcards.marker}\" == \"SSU\" ]; then\n        reference_fasta=\"{input.silva_general_fasta}\"\n    elif [ \"{wildcards.marker}\" == \"ITS1\" ]; then\n        reference_fasta=\"{input.unite_general_fasta}\"\n    else\n        echo \"Unsupported marker: {wildcards.marker}\"\n    fi\n\n    echo \"Running with {params.threads} threads.\"  \n    echo \"Used database for {wildcards.marker} in minimap2: $reference_fasta\" \n    \n    echo \"Starting minimap --version\"\n    minimap_version=$(minimap2 --version)\n    echo \"$minimap_version\"\n\n    minimap2 -cx map-ont -t {params.threads} \\\n            -N 10 -K 25M \\\n            $reference_fasta \\\n            {input.query} \\\n            -o {output}\n    \"\"\"\n```\n:::",
    "supporting": [
      "manual_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}