[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NanoITS",
    "section": "",
    "text": "NanoITS is a classifier for long-read Oxford Nanopore data of the eukaryotic 18S/SSU-ITS1-ITS2 operon.\nWhen giving the tool some nanopore long-read data it will:\nBelow you can find the full workflow:",
    "crumbs": [
      "NanoITS"
    ]
  },
  {
    "objectID": "source/manual.html",
    "href": "source/manual.html",
    "title": "Getting started",
    "section": "",
    "text": "To run the workflow conda/mamba and snakemake are required.\n\nTo install snakemake, check out the installation instructions found here. The workflow was so far tested with snakemake version 7.32.4.\nInformation about installing conda can be found here and for mamba here[https://mamba.readthedocs.io/en/latest/mamba-installation.html]."
  },
  {
    "objectID": "source/manual.html#prerequisites",
    "href": "source/manual.html#prerequisites",
    "title": "Getting started",
    "section": "",
    "text": "To run the workflow conda/mamba and snakemake are required.\n\nTo install snakemake, check out the installation instructions found here. The workflow was so far tested with snakemake version 7.32.4.\nInformation about installing conda can be found here and for mamba here[https://mamba.readthedocs.io/en/latest/mamba-installation.html]."
  },
  {
    "objectID": "source/manual.html#installation",
    "href": "source/manual.html#installation",
    "title": "Getting started",
    "section": "Installation",
    "text": "Installation\nFungomics can be installed via\n\ngit clone xx.git"
  },
  {
    "objectID": "source/manual.html#run-nanoits",
    "href": "source/manual.html#run-nanoits",
    "title": "Getting started",
    "section": "Run NanoITS",
    "text": "Run NanoITS\n\nGeneral setup\nTo run NanoITS you need to provide some information about the samples you want to analyse via the config.yaml file. Start with moving the config/config.yaml file to the folder with the fastq files you want to analyse and open the file with nano.\nThere are several things you can modify:\n\nProvide the project name in project: \"run_v1\". Your results will be generated in the folder you start the snakemake workflow in and the results will be generated in results/&lt;project_name&gt;. Your project name can contain letters, numbers _ and -. Please do not use other symbols, such as spaces.\nProvide a mapping file that describes the samples you want to analyse, i.e. samples_file: \"input/mapping.csv\". Here:\n\nsample: The names of your sample. This id will be used to label all files created in subsequent steps. In your sample names you can loose letters, numbers and -. Please do not use other symbols spaces, dots or underscores in your sample names.\nbarcode: The barcode ID. Can be empty as it is not actively used in the workflow\npath: Path to the fastq.gz files. The workflow accepts one file per barcode, so if you have more than one file merge this first using cat. Here, you can use a relative path (i.e. relative to the working directory you start the snakemake workflow in) or absolute.\n\nDecide what classifiers you want to use in classifiers: [\"minimap2\", \"kraken2\"]. Currently, two classifiers are implemented: (a) the alignment-based classifier minimap2 and (b) the kmer-based classifier kraken2. You can use both or either of the two classifiers.\nDecide what markers you want to analyse in markers: [\"SSU\", \"ITS1\"]. The workflow was developed for primers targetting both the SSU and ITS1 but the workflow will also run for either option selected and we plan to in the future extend the workflow to also accept the LSU rRNA gene.\nChange tool specific parameters: If desires, there are several parameters that can be changed by the users, such as the numbers of threads to use, the settings for the read filtering or the classification.\n\nExample mapping file:\nsample,barcode,path\nbc01,barcode01,/path/barcode01.fastq.gz\nbc02,barcode02,/path/barcode02.fastq.gz\n...\nSeveral steps of this workflow are quite heavy, so we recommend running this worklow on an HPC. If you want, you can of course try running it on a desktop computer.\n\n\nTest-run\nTo test whether the workflow will run successfully:\n\nProvide the path to where you installed NanoITS afer –s\nProvide the path to the config file after –configfile\nProvide the path to where you want snakemake to install all program dependencies. We recommend to install these into the folder in which you downloaded NanoITS but you can change this if desired\n\n\n#activate conda environment with your snakemake installation, i.e. \nmamba activate snakemake_7.32.4\n\n#test if everything runs as extended \nsnakemake --use-conda --cores 1 -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile --configfile config/config.yaml --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  --rerun-incomplete --nolock -np \n\nIf the test-run was successful you can run snakemake interactively with:\n\nsnakemake --use-conda --cores 1 -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile --configfile config/config.yaml --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  --rerun-incomplete --nolock\n\nOr via a slurm HPC, such as UvA´s crunchomics server by doing the following edits in jobscript.sh:\n\nExchange the line that utilizes snakemake with the line of code that you used for the testrun to ensure that all paths are set correctly.\nAdjust the number of CPUs as needed\n\n\nsbatch jobscript.sh\n\n\n\nDevelopment stage\n\nSetup snakemake\n\nmamba create -n snakemake_7.32.4 -c conda-forge -c bioconda snakemake=7.32.4\n\n\nconda deactivate\nexport LC_ALL=en_US.UTF-8\n\n#set working directory\nwdir=\"/home/ndombro/personal/snakemake_workflows/NanoITS\"\n\ncd $wdir \n\nmamba activate snakemake_7.32.4\n\n\n\nOrganize folder structure\n\n#organize folders\n# mkdir -p workflow/scripts\n# mkdir workflow/envs\n# mkdir workflow/rules\n# mkdir workflow/report\n\nmkdir docs\nmkdir config\n\nmkdir -p input/raw\nmkdir -p input/combined\n\necho -e 'barcode02\\nbarcode05' &gt; input/samples.txt\necho -e \"test\" &gt;  workflow/report/workflow.rst\n\ncp /home/ndombro/personal/snakemake_workflows/fungomics/config/config.yaml config\ncp ~/personal/snakemake_workflows/fungomics/jobscript.sh .\n\n\n\nGet yaml for existing workflows\n\nconda activate nanopore\nconda env export &gt; workflow/envs/nanopore.yaml\nconda deactivate\n\nconda activate r_for_amplicon\nconda env export &gt; workflow/envs/r_for_amplicon.yaml \nconda deactivate\n\n\n\nPrepare db folder\nWe will prepare a db folder that can be downloaded from zenodo. The reason for doing that is because unite can not be downloaded via wget since one needs to fill out a form. Therefore, we will the already prepared databases via zenodo.\n\ncd /home/ndombro/personal/for_zenodo/fungomics/db\n\nmkdir unite/general\nmkdir silva/general\n\n#db used by minimap\ncp ~/personal/projects/fungomics/mangrove/db/unite/general_release/unite-ref-seqs.fna unite/general/\ncp ~/personal/projects/fungomics/mangrove/db/unite/general_release/unite-ref-taxonomy.txt unite/general/\n\ncp ~/personal/projects/fungomics/mangrove/db/silva/general/silva-ref-taxonomy.txt silva/general/\ncp ~/personal/projects/fungomics/mangrove/db/silva/general/silva-ref.fasta silva/general/\n\n#db used by kraken\ncp -r ~/personal/projects/fungomics/mangrove/db/unite/general_release/db_kraken/* unite/\n\ncp -r ~/personal/projects/fungomics/mangrove/db/silva/kraken silva/\n\n#clean up symbolic links\ncd silva/kraken\nrm -r taxonomy\ncp -r /home/ndombro/personal/projects/fungomics/mangrove/db/ncbi/taxonomy/ .\n\ncd ../../..\n\n#compress\ntar -zcvf db.tar.gz db\n\n\n\nPrepare test data\n\nln -s /zfs/omics/personal/ndombro/projects/fungomics/mangrove/data/fastq_pass/barcode02/ ${PWD}/input/raw\nln -s /zfs/omics/personal/ndombro/projects/fungomics/mangrove/data/fastq_pass/barcode05/ ${PWD}/input/raw\n\nfor i in `cat input/samples.txt`; do\n  cat input/raw/${i}/*fastq.gz &gt; input/combined/${i}.fastq.gz\ndone\n\n# cp input/raw/barcode02/aps797_pass_barcode02_72052f83_778f88e7_2.fastq.gz input/combined/barcode02.fastq.gz\n# cp input/raw/barcode05/aps797_pass_barcode05_72052f83_778f88e7_2.fastq.gz input/combined/barcode05.fastq.gz\n\necho -e 'sample,barcode,path\\nbc02,barcode02,input/combined/barcode02.fastq.gz\\nbc05,barcode05,input/combined/barcode05.fastq.gz' &gt; input/mapping.txt\n\necho $(zcat input/combined/barcode02.fastq.gz | wc -l)/4|bc\necho $(zcat input/combined/barcode05.fastq.gz | wc -l)/4|bc\n\necho $(zcat input/raw/barcode02/*fastq.gz | wc -l)/4|bc\n\n\n#dry-run\nsnakemake --use-conda --cores 1 --configfile config/config.yaml --conda-prefix workflow/.snakemake/conda  --rerun-incomplete --nolock -np \n\n#full run\nsbatch jobscript.sh\n\n#generate report\nsnakemake --report report.html --configfile config/config.yaml \n\nRscript {my_basedir}/scripts/otu_analysis.R results/{project}/classification/*/${{i}}.merged.outmat.tsv ${{i}} results/{project}\n\n\nfile_paths &lt;- c(\"results/test_run/classification/minimap2/SSU.merged.outmat.tsv\", \"results/test_run/classification/kraken2/SSU.merged.outmat.tsv\")\nmarker_id &lt;- \"SSU\"\n\n\n\nresults/test_run/classification/minimap2/SSU.merged.outmat.tsv\nresults/test_run/classification/kraken2/SSU.merged.outmat.tsv"
  },
  {
    "objectID": "source/manual.html#other-notes",
    "href": "source/manual.html#other-notes",
    "title": "Getting started",
    "section": "Other notes",
    "text": "Other notes\n\nComment on fickle script:\nIf you adjust something in the file path then workflow/scripts/tomat.py needs to be adjusted, esp this part:\n\npathfile=\" results/test_run/classification/minimap2/bc02/bc02_ITS1_minimap2.taxlist\"\n\nparts = pathfile.split('/')\ndir = '/'.join(parts[:4])\n\nproject = parts[1]\nmethod = parts[3]\nsample = parts[4]\nmarker = parts[5].split('_')[1]\n\noutpath = f\"{dir}/{sample}/{sample}_{marker}_{method}\"\n\nIn workflow/scripts/merge_otu_tables.py adjust:\n\nbarcode = os.path.join(root, dir).split(os.path.sep)[4]\n\n\n\nNotes on how the taxonomy files were generated\nCurrently, the workflow supports two databases:\n\nUnite for ITS1 marker gene analyses (linked to the Unite taxonomy)\nSilva for 16S/18S rRNA gene analyes (linked to the NCBI taxonomy)\n\nBelow, you find the code to:\n\nDownload the sequences for both databases\nDownload a taxonomy mapping file (Silva) or generate a custom mapping file (Unite)\nParse these two to be suited for use in the Minimap2 and Kraken2 classification step\n\n\n\nUnite (ITS)\n\nDownload db\nUnite can not be downloaded via wget since one needs to receive a download link via mail. But the data was downloaded via the web browser on 0210223 from this website and this website then uploaded to crunchomics into the working directory and this folder db/unite/general_release:\n\nmkdir -p db/unite/general_release\n\ncd db/unite/general_release\ntar -zxvf sh_general_release_all_18.07.2023.tgz\ncd ../..\n\n\n\nParse db for minimap2\nArguments for shorten_fasta_headers.py:\n\n--header_part INT: The unite file has a longer header with different elements separated by “|”, here we just decide which element after separation we want to keep.\n\n\n#shorten header \npython scripts/shorten_fasta_headers.py -i db/unite/general_release/sh_general_release_dynamic_all_18.07.2023.fasta -o db/unite/general_release/unite-ref-seqs.fna --header_part 3\n\n#check if headers are unique\ngrep \"&gt;\" db/unite/general_release/unite-ref-seqs.fna | sort | uniq -d\n\n#make a taxon mapping file \npython scripts/parse_unite_tax.py -i db/unite/general_release/sh_general_release_dynamic_all_18.07.2023.fasta  -o db/unite/general_release/unite-ref-taxonomy.txt\n\n\n\nParse db for kraken2\nNotice: To convert the fasta header to a taxonomy mapping file, a perl script that is part of kraken2 was used. This script can be found in the scripts folder but originally comes from here and was downloaded with wget -P scripts https://raw.githubusercontent.com/DerrickWood/kraken2/master/scripts/build_rdp_taxonomy.pl.\n\nconda activate nanopore\ncd db/unite/general_release\n\nmkdir -p ./db_kraken\n\n#convert fasta header into something that can be used by kraken2 for db generation\nperl -pe '\n  s/&gt;([^\\|]+)\\|([^\\|]+)\\|([^\\|]+)/&gt;$2 $1; $3/;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Fungi/\\tLineage=Root;rootrank;Fungi;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Viridiplantae/\\tLineage=Root;rootrank;Viridiplantae;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Eukaryota_kgd_Incertae_sedis/\\tLineage=Root;rootrank;Eukaryota_kgd_Incertae_sedis;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Alveolata/\\tLineage=Root;rootrank;Alveolata;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Amoebozoa/\\tLineage=Root;rootrank;Amoebozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Apusozoa/\\tLineage=Root;rootrank;Apusozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Choanoflagellozoa/\\tLineage=Root;rootrank;Choanoflagellozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Cryptista/\\tLineage=Root;rootrank;Cryptista;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Euglenozoa/\\tLineage=Root;rootrank;Euglenozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Glaucocystoplantae/\\tLineage=Root;rootrank;Glaucocystoplantae;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Haptista/\\tLineage=Root;rootrank;Haptista;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Heterolobosa/\\tLineage=Root;rootrank;Heterolobosa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Ichthyosporia/\\tLineage=Root;rootrank;Ichthyosporia;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Metazoa/\\tLineage=Root;rootrank;Metazoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Planomonada/\\tLineage=Root;rootrank;Planomonada;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Protista/\\tLineage=Root;rootrank;Protista;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Rhizaria/\\tLineage=Root;rootrank;Rhizaria;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Rhodoplantae/\\tLineage=Root;rootrank;Rhodoplantae;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Stramenopila/\\tLineage=Root;rootrank;Stramenopila;domain/g;\n  s/\\|/ /g;\n  s/;s__*.*//g;\n  s/p__([a-zA-Z0-9_ -]*)/\\1;phylum/g;\n  s/c__([a-zA-Z0-9_ -]*)/\\1;class/g;\n  s/o__([a-zA-Z0-9_ -]*)/\\1;order/g;\n  s/f__([a-zA-Z0-9_ -]*)/\\1;family/g;\n  s/g__([a-zA-Z0-9_ -]*)/\\1;genus/g;\n  s/;$//g'  sh_general_release_dynamic_all_18.07.2023.fasta &gt; ./db_kraken/kunite.fasta\n\n#generate taxonomy mapping files based on the fasta header\nperl ../../../scripts/build_rdp_taxonomy.pl ./db_kraken/kunite.fasta\n\n#organize folder structure\nmkdir -p ./db_kraken/library \nmv ./db_kraken/kunite.fasta ./db_kraken/library/unite.fna\nmkdir -p ./db_kraken/taxonomy\nmv names.dmp nodes.dmp ./db_kraken/taxonomy\nmv seqid2taxid.map ./db_kraken\n\n#buld db\nkraken2-build --build --db db_kraken\n\n#convert to taxID_to_string mapping\ncd db_kraken\n\ntaxonkit lineage --data-dir ./taxonomy &lt;(awk '{print $2}' seqid2taxid.map |\\\n sort | uniq) | \\\n taxonkit reformat -r NA -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir ./taxonomy | \\\n awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' &gt; taxID_to_tax.txt \n\ncd ../../../..\n\nconda deactivate\n\n\n\n\nSilva (SSU)\n\nDownload data and parse for general use\n\nmkdir -p db/silva/general/\nmkdir -p db/ncbi/taxonomy \n\n#download ncbi taxonomy \nwget -N ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz\ntar zxf taxdump.tar.gz -C db/ncbi/taxonomy\nrm taxdump.tar.gz\n\n#download SILVA 16S sequences\ncd db/silva/general \nwget https://www.arb-silva.de/fileadmin/silva_databases/current/Exports/SILVA_138.1_SSURef_NR99_tax_silva.fasta.gz\ngzip -d *.gz\n\n#clean fasta header\ncut -d ' ' -f1 SILVA_138.1_SSURef_NR99_tax_silva.fasta &gt; silva-ref.fasta\n\n#get the silva id to ncbi taxID mapping file (notice: the mapping has a lot of nas, discard)\nwget https://www.arb-silva.de/fileadmin/silva_databases/current/Exports/taxonomy/ncbi/taxmap_embl-ebi_ena_ssu_ref_nr99_138.1.txt.gz -q -O - | gzip -d -c - |  awk '{print $1\".\"$2\".\"$3\"\\t\"$(NF)}' &gt; ref-tax.map\n\ntaxonkit lineage --data-dir db/ncbi/taxonomy &lt;(awk '{print $2}' ref-tax.map | sort | uniq) | \\\n  taxonkit reformat -r NA -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir db/ncbi/taxonomy | \\\n  awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' &gt; taxID_to_tax.txt \n\n#merge the info, for missing hits add a string with NAs\nawk -F'\\t' -v OFS='\\t' 'NR==FNR{taxonomy[$1]=$2; next} {print $1, ($2 in taxonomy) ? taxonomy[$2] : \"D_0__NA;D_1__NA;D_2__NA;D_3__NA;D_4__NA;D_5__NA;D_6__NA\"}' taxID_to_tax.txt  ref-tax.map &gt; silva-ref-taxonomy.txt\n\n#merge things: taxID_to_tax.txt  1 and ref-tax.map 2\nLC_ALL=C join -1 2 -2 1 -t $'\\t' &lt;(LC_ALL=C sort -k 2 ref-tax.map) &lt;(LC_ALL=C sort -k1 taxID_to_tax.txt | sed \"s/ /_/g\") | awk -v OFS=\"\\t\" '{print $2, $3}' &gt; sequence_to_tax.txt\n\ncd ../..\n\n\n\nParse and download db for minimap\nThe code below is sufficient to read the data into minimap2.\n\n\nParse and download db for kraken\nThe code below generates a database to run kraken. Notice, that this does not use the default Silva database with links to the silva taxonomy but generates a database with the SILVA 16S rRNA gene sequences and labels based on the NCBI taxonomy.\n\nconda activate nanopore \n\nmkdir -p db/silva/kraken\nmkdir -p db/silva/kraken/library \n\n#based on the Silva files generate a mapping file for a fasta header suitable for kraken\npython scripts/generate_header_for_kraken.py -i db/silva/general/sequence_to_tax.txt -o db/silva/general/sequence_to_krakenFmanual.txt\n\n#use mapping file to convert the fasta header into a kraken2 compatible format\npython scripts/convert_fasta_for_kraken.py -f db/silva/general/silva-ref.fasta -m db/silva/general/sequence_to_krakenFmanual.txt -o db/silva/general/silva-ref-modified.fasta\n\n#Add NAs for unassigned taxa and convert to single line fasta with T instead of Us\nsed 's/;;/;NA/g' db/silva/general/silva-ref-modified.fasta | \\\n  sed '/^&gt;/!s/U/T/g'| \\\n  awk '/^&gt;/ {if(NR!=1) printf(\"\\n%s\\n\",$0); else printf(\"%s\\n\",$0); next; } { printf(\"%s\",$0);} END {printf(\"\\n\");}' &gt; db/silva/kraken/library/silva-ref-modified.fna\n\n#add symlink to ncbi taxonomy \nln -s $PWD/db/ncbi/taxonomy $PWD/db/silva/kraken/taxonomy\n\n#generate seqid2taxid.map that links the seqID to the ncbi tax ids\ncp db/silva/general/ref-tax.map db/silva/kraken/seqid2taxid.map\n\n#generate db\n#added fast-build option because run seemed stalled\nsrun -n 1 --cpus-per-task 32 kraken2-build --build --fast-build --threads 32 --db db/silva/kraken  \n\n#generate a mapping file for the sequence ID and tax string\ntaxonkit lineage --data-dir db/silva/kraken_test2/taxonomy &lt;(awk '{print $1}' db/silva/kraken/taxonomy/names.dmp |\\\n sort | uniq) | \\\n taxonkit reformat -r NA -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir db/silva/kraken/taxonomy | \\\n awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' &gt; db/silva/kraken/taxID_to_tax.txt \n\n#reformat a string to be recognized by a downstream script\nsed -i 's|D_0__;D_1__;D_2__;D_3__;D_4__;D_5__;D_6__|D_0__NA;D_1__NA;D_2__NA;D_3__NA;D_4__NA;D_5__NA;D_6__NA|g' db/silva/kraken/taxID_to_tax.txt\n\ncd ../../../..\n\nconda deactivate\n\n\nrule download_dbs:\n    output:\n        silva_general_fasta = os.path.join(my_basedir,\"db/silva/general/silva-ref.fasta\"),\n        silva_general_tax = os.path.join(my_basedir, \"db/silva/general/silva-ref-taxonomy.txt\"),\n\n        unite_general_fasta = os.path.join(my_basedir,\"db/unite/general/unite-ref-seqs.fna\"),\n        unite_general_tax = os.path.join(my_basedir, \"db/unite/general/unite-ref-taxonomy.txt\"),\n\n        silva_kraken_db = os.path.join(my_basedir, \"/db/silva/kraken/\"),\n        unite_kraken_db = os.path.join(my_basedir, \"/db/unite/kraken/\")\n    log:\n        \"logs/run_download_db.log\"\n    conda:\n        \"../envs/nanopore.yaml\"\n    threads: 1\n    shell: \"\"\"\n    wget -q -O db.tar.gz https://zenodo.org/records/10052996/files/db.tar.gz?download=1\n    \n    #extract general silva db\n    tar -zxvf db.tar.gz -C {my_basedir}/db/silva/general/ \\\n        --wildcards --no-anchored 'silva-ref.fasta' -O &gt; {output.silva_general_fasta}\n\n    tar -zxvf db.tar.gz -C {my_basedir}/db/silva/general/ \\\n        --wildcards --no-anchored 'silva-ref-taxonomy.txt' -O &gt; {output.silva_general_tax}\n\n    #extract general unite db\n    tar -zxvf db.tar.gz -C {my_basedir}/db/unite/general/ \\\n        --wildcards --no-anchored 'unite-ref-seqs.fna' -O &gt; {output.unite_general_fasta}\n\n    tar -zxvf db.tar.gz -C {my_basedir}/db/unite/general/ \\\n        --wildcards --no-anchored 'unite-ref-taxonomy.txt' -O &gt; {output.unite_general_tax}\n\n    #extract kraken db for silva and unite\n    tar -zxvf db.tar.gz -C {output.silva_kraken_db} --wildcards --no-anchored 'db/silva/kraken/*' --strip-components=3\n    tar -zxvf db.tar.gz -C {output.unite_kraken_db} --wildcards --no-anchored 'db/unite/kraken/*' --strip-components=3\n\n    \"\"\"\n\n\nrule run_minimap_SSU:\n    input:\n        query = \"results/itsx/{sample}/{sample}_{marker}_final.fasta\",\n        silva_general_fasta = os.path.join(my_basedir, \"db/silva/general/silva-ref.fasta\"),\n        unite_general_fasta = os.path.join(my_basedir, \"db/unite/general/unite-ref-seqs.fna\")\n    output:\n        \"results/classification/minimap2/{sample}/{sample}_{marker}_minimap2.paf\"\n    params:\n        threads=config[\"minimap2\"][\"threads\"]\n    resources:\n        mem_mb = lambda wildcards, attempt: attempt * config[\"minimap2\"][\"memory\"]\n    log:\n        \"logs/run_minimap2_{sample}_{marker}.log\"\n    conda:\n        \"../envs/nanopore.yaml\"\n    shell: \"\"\"\n\n    if [ \"{wildcards.marker}\" == \"SSU\" ]; then\n        reference_fasta=\"{input.silva_general_fasta}\"\n    elif [ \"{wildcards.marker}\" == \"ITS1\" ]; then\n        reference_fasta=\"{input.unite_general_fasta}\"\n    else\n        echo \"Unsupported marker: {wildcards.marker}\"\n    fi\n\n    echo \"Running with {params.threads} threads.\"  \n    echo \"Used database for {wildcards.marker} in minimap2: $reference_fasta\" \n    \n    echo \"Starting minimap --version\"\n    minimap_version=$(minimap2 --version)\n    echo \"$minimap_version\"\n\n    minimap2 -cx map-ont -t {params.threads} \\\n            -N 10 -K 25M \\\n            $reference_fasta \\\n            {input.query} \\\n            -o {output}\n    \"\"\""
  },
  {
    "objectID": "source/run_nanoITS.html",
    "href": "source/run_nanoITS.html",
    "title": "Run NanoITS",
    "section": "",
    "text": "NanoITS will take de-multiplexed and compressed fastq files as input and generate OTU tables and some summary statistics as output.\nAs input, NanoITS takes a single fastq.gz file per sample.",
    "crumbs": [
      "Run NanoITS"
    ]
  },
  {
    "objectID": "source/run_nanoITS.html#run-nanoits",
    "href": "source/run_nanoITS.html#run-nanoits",
    "title": "Run NanoITS",
    "section": "Run NanoITS",
    "text": "Run NanoITS\n\nDry-run\nTo test whether the workflow is defined properly do a dry-run first. To do this, change part of the snakemake command as follows:\n\nProvide the path to where you installed NanoITS after --s\nProvide the path to the edited config file after --configfile\nProvide the path to where you want snakemake to install all program dependencies after --conda-prefix. We recommend to install these into the folder in which you downloaded NanoITS but you can change this if desired\n\n\n#activate conda environment with your snakemake installation, i.e. \nmamba activate snakemake_7.32.4\n\n#test if everything runs as extended (edit as described above)\nsnakemake --use-conda --cores 1 \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile \\\n  --configfile config/config.yaml \\\n  --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  \\\n  --rerun-incomplete --nolock -np \n\n\n\nRun NanoITS interactively\nIf the dry-run was successful you can run snakemake interactively with the following command. Adjust the cores according to your system.\n\nsnakemake --use-conda --cores 1 \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile \\\n  --configfile config/config.yaml \\\n  --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda \\\n  --rerun-incomplete --nolock\n\n\n\nRun NanoITS on Crunchomics (Uva-specific option)\nIf you are a student or staff at the University of Amsterdam and part of the IBED department, you can also run NanoITS via the Crunchomics HPC. Detailed instructions how to do thos can be found here.",
    "crumbs": [
      "Run NanoITS"
    ]
  },
  {
    "objectID": "source/run_nanoITS.html#other-notes",
    "href": "source/run_nanoITS.html#other-notes",
    "title": "Run NanoITS",
    "section": "Other notes",
    "text": "Other notes\n\nComment on fickle script:\nIf you adjust something in the file path then workflow/scripts/tomat.py needs to be adjusted, esp this part:\n\npathfile=\" results/test_run/classification/minimap2/bc02/bc02_ITS1_minimap2.taxlist\"\n\nparts = pathfile.split('/')\ndir = '/'.join(parts[:4])\n\nproject = parts[1]\nmethod = parts[3]\nsample = parts[4]\nmarker = parts[5].split('_')[1]\n\noutpath = f\"{dir}/{sample}/{sample}_{marker}_{method}\"\n\nIn workflow/scripts/merge_otu_tables.py adjust:\n\nbarcode = os.path.join(root, dir).split(os.path.sep)[4]\n\n\n\nNotes on how the taxonomy files were generated\nCurrently, the workflow supports two databases:\n\nUnite for ITS1 marker gene analyses (linked to the Unite taxonomy)\nSilva for 16S/18S rRNA gene analyes (linked to the NCBI taxonomy)\n\nBelow, you find the code to:\n\nDownload the sequences for both databases\nDownload a taxonomy mapping file (Silva) or generate a custom mapping file (Unite)\nParse these two to be suited for use in the Minimap2 and Kraken2 classification step\n\n\n\nUnite (ITS)\n\nDownload db\nUnite can not be downloaded via wget since one needs to receive a download link via mail. But the data was downloaded via the web browser on 0210223 from this website and this website then uploaded to crunchomics into the working directory and this folder db/unite/general_release:\n\nmkdir -p db/unite/general_release\n\ncd db/unite/general_release\ntar -zxvf sh_general_release_all_18.07.2023.tgz\ncd ../..\n\n\n\nParse db for minimap2\nArguments for shorten_fasta_headers.py:\n\n--header_part INT: The unite file has a longer header with different elements separated by “|”, here we just decide which element after separation we want to keep.\n\n\n#shorten header \npython scripts/shorten_fasta_headers.py -i db/unite/general_release/sh_general_release_dynamic_all_18.07.2023.fasta -o db/unite/general_release/unite-ref-seqs.fna --header_part 3\n\n#check if headers are unique\ngrep \"&gt;\" db/unite/general_release/unite-ref-seqs.fna | sort | uniq -d\n\n#make a taxon mapping file \npython scripts/parse_unite_tax.py -i db/unite/general_release/sh_general_release_dynamic_all_18.07.2023.fasta  -o db/unite/general_release/unite-ref-taxonomy.txt\n\n\n\nParse db for kraken2\nNotice: To convert the fasta header to a taxonomy mapping file, a perl script that is part of kraken2 was used. This script can be found in the scripts folder but originally comes from here and was downloaded with wget -P scripts https://raw.githubusercontent.com/DerrickWood/kraken2/master/scripts/build_rdp_taxonomy.pl.\n\nconda activate nanopore\ncd db/unite/general_release\n\nmkdir -p ./db_kraken\n\n#convert fasta header into something that can be used by kraken2 for db generation\nperl -pe '\n  s/&gt;([^\\|]+)\\|([^\\|]+)\\|([^\\|]+)/&gt;$2 $1; $3/;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Fungi/\\tLineage=Root;rootrank;Fungi;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Viridiplantae/\\tLineage=Root;rootrank;Viridiplantae;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Eukaryota_kgd_Incertae_sedis/\\tLineage=Root;rootrank;Eukaryota_kgd_Incertae_sedis;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Alveolata/\\tLineage=Root;rootrank;Alveolata;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Amoebozoa/\\tLineage=Root;rootrank;Amoebozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Apusozoa/\\tLineage=Root;rootrank;Apusozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Choanoflagellozoa/\\tLineage=Root;rootrank;Choanoflagellozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Cryptista/\\tLineage=Root;rootrank;Cryptista;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Euglenozoa/\\tLineage=Root;rootrank;Euglenozoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Glaucocystoplantae/\\tLineage=Root;rootrank;Glaucocystoplantae;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Haptista/\\tLineage=Root;rootrank;Haptista;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Heterolobosa/\\tLineage=Root;rootrank;Heterolobosa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Ichthyosporia/\\tLineage=Root;rootrank;Ichthyosporia;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Metazoa/\\tLineage=Root;rootrank;Metazoa;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Planomonada/\\tLineage=Root;rootrank;Planomonada;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Protista/\\tLineage=Root;rootrank;Protista;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Rhizaria/\\tLineage=Root;rootrank;Rhizaria;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Rhodoplantae/\\tLineage=Root;rootrank;Rhodoplantae;domain/g;\n  s/\\|re[a-z]s_*[a-z]*\\|k__Stramenopila/\\tLineage=Root;rootrank;Stramenopila;domain/g;\n  s/\\|/ /g;\n  s/;s__*.*//g;\n  s/p__([a-zA-Z0-9_ -]*)/\\1;phylum/g;\n  s/c__([a-zA-Z0-9_ -]*)/\\1;class/g;\n  s/o__([a-zA-Z0-9_ -]*)/\\1;order/g;\n  s/f__([a-zA-Z0-9_ -]*)/\\1;family/g;\n  s/g__([a-zA-Z0-9_ -]*)/\\1;genus/g;\n  s/;$//g'  sh_general_release_dynamic_all_18.07.2023.fasta &gt; ./db_kraken/kunite.fasta\n\n#generate taxonomy mapping files based on the fasta header\nperl ../../../scripts/build_rdp_taxonomy.pl ./db_kraken/kunite.fasta\n\n#organize folder structure\nmkdir -p ./db_kraken/library \nmv ./db_kraken/kunite.fasta ./db_kraken/library/unite.fna\nmkdir -p ./db_kraken/taxonomy\nmv names.dmp nodes.dmp ./db_kraken/taxonomy\nmv seqid2taxid.map ./db_kraken\n\n#buld db\nkraken2-build --build --db db_kraken\n\n#convert to taxID_to_string mapping\ncd db_kraken\n\ntaxonkit lineage --data-dir ./taxonomy &lt;(awk '{print $2}' seqid2taxid.map |\\\n sort | uniq) | \\\n taxonkit reformat -r NA -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir ./taxonomy | \\\n awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' &gt; taxID_to_tax.txt \n\ncd ../../../..\n\nconda deactivate\n\n\n\n\nSilva (SSU)\n\nDownload data and parse for general use\n\nmkdir -p db/silva/general/\nmkdir -p db/ncbi/taxonomy \n\n#download ncbi taxonomy \nwget -N ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz\ntar zxf taxdump.tar.gz -C db/ncbi/taxonomy\nrm taxdump.tar.gz\n\n#download SILVA 16S sequences\ncd db/silva/general \nwget https://www.arb-silva.de/fileadmin/silva_databases/current/Exports/SILVA_138.1_SSURef_NR99_tax_silva.fasta.gz\ngzip -d *.gz\n\n#clean fasta header\ncut -d ' ' -f1 SILVA_138.1_SSURef_NR99_tax_silva.fasta &gt; silva-ref.fasta\n\n#get the silva id to ncbi taxID mapping file (notice: the mapping has a lot of nas, discard)\nwget https://www.arb-silva.de/fileadmin/silva_databases/current/Exports/taxonomy/ncbi/taxmap_embl-ebi_ena_ssu_ref_nr99_138.1.txt.gz -q -O - | gzip -d -c - |  awk '{print $1\".\"$2\".\"$3\"\\t\"$(NF)}' &gt; ref-tax.map\n\ntaxonkit lineage --data-dir db/ncbi/taxonomy &lt;(awk '{print $2}' ref-tax.map | sort | uniq) | \\\n  taxonkit reformat -r NA -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir db/ncbi/taxonomy | \\\n  awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' &gt; taxID_to_tax.txt \n\n#merge the info, for missing hits add a string with NAs\nawk -F'\\t' -v OFS='\\t' 'NR==FNR{taxonomy[$1]=$2; next} {print $1, ($2 in taxonomy) ? taxonomy[$2] : \"D_0__NA;D_1__NA;D_2__NA;D_3__NA;D_4__NA;D_5__NA;D_6__NA\"}' taxID_to_tax.txt  ref-tax.map &gt; silva-ref-taxonomy.txt\n\n#merge things: taxID_to_tax.txt  1 and ref-tax.map 2\nLC_ALL=C join -1 2 -2 1 -t $'\\t' &lt;(LC_ALL=C sort -k 2 ref-tax.map) &lt;(LC_ALL=C sort -k1 taxID_to_tax.txt | sed \"s/ /_/g\") | awk -v OFS=\"\\t\" '{print $2, $3}' &gt; sequence_to_tax.txt\n\ncd ../..\n\n\n\nParse and download db for minimap\nThe code below is sufficient to read the data into minimap2.\n\n\nParse and download db for kraken\nThe code below generates a database to run kraken. Notice, that this does not use the default Silva database with links to the silva taxonomy but generates a database with the SILVA 16S rRNA gene sequences and labels based on the NCBI taxonomy.\n\nconda activate nanopore \n\nmkdir -p db/silva/kraken\nmkdir -p db/silva/kraken/library \n\n#based on the Silva files generate a mapping file for a fasta header suitable for kraken\npython scripts/generate_header_for_kraken.py -i db/silva/general/sequence_to_tax.txt -o db/silva/general/sequence_to_krakenFmanual.txt\n\n#use mapping file to convert the fasta header into a kraken2 compatible format\npython scripts/convert_fasta_for_kraken.py -f db/silva/general/silva-ref.fasta -m db/silva/general/sequence_to_krakenFmanual.txt -o db/silva/general/silva-ref-modified.fasta\n\n#Add NAs for unassigned taxa and convert to single line fasta with T instead of Us\nsed 's/;;/;NA/g' db/silva/general/silva-ref-modified.fasta | \\\n  sed '/^&gt;/!s/U/T/g'| \\\n  awk '/^&gt;/ {if(NR!=1) printf(\"\\n%s\\n\",$0); else printf(\"%s\\n\",$0); next; } { printf(\"%s\",$0);} END {printf(\"\\n\");}' &gt; db/silva/kraken/library/silva-ref-modified.fna\n\n#add symlink to ncbi taxonomy \nln -s $PWD/db/ncbi/taxonomy $PWD/db/silva/kraken/taxonomy\n\n#generate seqid2taxid.map that links the seqID to the ncbi tax ids\ncp db/silva/general/ref-tax.map db/silva/kraken/seqid2taxid.map\n\n#generate db\n#added fast-build option because run seemed stalled\nsrun -n 1 --cpus-per-task 32 kraken2-build --build --fast-build --threads 32 --db db/silva/kraken  \n\n#generate a mapping file for the sequence ID and tax string\ntaxonkit lineage --data-dir db/silva/kraken_test2/taxonomy &lt;(awk '{print $1}' db/silva/kraken/taxonomy/names.dmp |\\\n sort | uniq) | \\\n taxonkit reformat -r NA -f \"{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}\" --data-dir db/silva/kraken/taxonomy | \\\n awk 'BEGIN { FS = OFS = \"\\t\" } {print $1,\"D_0__\"$3\";D_1__\"$4\";D_2__\"$5\";D_3__\"$6\";D_4__\"$7\";D_5__\"$8\";D_6__\"$9}' &gt; db/silva/kraken/taxID_to_tax.txt \n\n#reformat a string to be recognized by a downstream script\nsed -i 's|D_0__;D_1__;D_2__;D_3__;D_4__;D_5__;D_6__|D_0__NA;D_1__NA;D_2__NA;D_3__NA;D_4__NA;D_5__NA;D_6__NA|g' db/silva/kraken/taxID_to_tax.txt\n\ncd ../../../..\n\nconda deactivate\n\n\nrule download_dbs:\n    output:\n        silva_general_fasta = os.path.join(my_basedir,\"db/silva/general/silva-ref.fasta\"),\n        silva_general_tax = os.path.join(my_basedir, \"db/silva/general/silva-ref-taxonomy.txt\"),\n\n        unite_general_fasta = os.path.join(my_basedir,\"db/unite/general/unite-ref-seqs.fna\"),\n        unite_general_tax = os.path.join(my_basedir, \"db/unite/general/unite-ref-taxonomy.txt\"),\n\n        silva_kraken_db = os.path.join(my_basedir, \"/db/silva/kraken/\"),\n        unite_kraken_db = os.path.join(my_basedir, \"/db/unite/kraken/\")\n    log:\n        \"logs/run_download_db.log\"\n    conda:\n        \"../envs/nanopore.yaml\"\n    threads: 1\n    shell: \"\"\"\n    wget -q -O db.tar.gz https://zenodo.org/records/10052996/files/db.tar.gz?download=1\n    \n    #extract general silva db\n    tar -zxvf db.tar.gz -C {my_basedir}/db/silva/general/ \\\n        --wildcards --no-anchored 'silva-ref.fasta' -O &gt; {output.silva_general_fasta}\n\n    tar -zxvf db.tar.gz -C {my_basedir}/db/silva/general/ \\\n        --wildcards --no-anchored 'silva-ref-taxonomy.txt' -O &gt; {output.silva_general_tax}\n\n    #extract general unite db\n    tar -zxvf db.tar.gz -C {my_basedir}/db/unite/general/ \\\n        --wildcards --no-anchored 'unite-ref-seqs.fna' -O &gt; {output.unite_general_fasta}\n\n    tar -zxvf db.tar.gz -C {my_basedir}/db/unite/general/ \\\n        --wildcards --no-anchored 'unite-ref-taxonomy.txt' -O &gt; {output.unite_general_tax}\n\n    #extract kraken db for silva and unite\n    tar -zxvf db.tar.gz -C {output.silva_kraken_db} --wildcards --no-anchored 'db/silva/kraken/*' --strip-components=3\n    tar -zxvf db.tar.gz -C {output.unite_kraken_db} --wildcards --no-anchored 'db/unite/kraken/*' --strip-components=3\n\n    \"\"\"\n\n\nrule run_minimap_SSU:\n    input:\n        query = \"results/itsx/{sample}/{sample}_{marker}_final.fasta\",\n        silva_general_fasta = os.path.join(my_basedir, \"db/silva/general/silva-ref.fasta\"),\n        unite_general_fasta = os.path.join(my_basedir, \"db/unite/general/unite-ref-seqs.fna\")\n    output:\n        \"results/classification/minimap2/{sample}/{sample}_{marker}_minimap2.paf\"\n    params:\n        threads=config[\"minimap2\"][\"threads\"]\n    resources:\n        mem_mb = lambda wildcards, attempt: attempt * config[\"minimap2\"][\"memory\"]\n    log:\n        \"logs/run_minimap2_{sample}_{marker}.log\"\n    conda:\n        \"../envs/nanopore.yaml\"\n    shell: \"\"\"\n\n    if [ \"{wildcards.marker}\" == \"SSU\" ]; then\n        reference_fasta=\"{input.silva_general_fasta}\"\n    elif [ \"{wildcards.marker}\" == \"ITS1\" ]; then\n        reference_fasta=\"{input.unite_general_fasta}\"\n    else\n        echo \"Unsupported marker: {wildcards.marker}\"\n    fi\n\n    echo \"Running with {params.threads} threads.\"  \n    echo \"Used database for {wildcards.marker} in minimap2: $reference_fasta\" \n    \n    echo \"Starting minimap --version\"\n    minimap_version=$(minimap2 --version)\n    echo \"$minimap_version\"\n\n    minimap2 -cx map-ont -t {params.threads} \\\n            -N 10 -K 25M \\\n            $reference_fasta \\\n            {input.query} \\\n            -o {output}\n    \"\"\""
  },
  {
    "objectID": "source/prerequisites.html",
    "href": "source/prerequisites.html",
    "title": "Getting started",
    "section": "",
    "text": "To run the workflow you need to have mamba (conda also works) and snakemake installed.\nAll other software required by NanoITs will be installed by snakemake for you in a conda environment.\n\n\nInformation about installing mamba can be found here and for conda can be found here.\n\n\n\nAfter having installed conda/mamba you can install snakemake as follows. Notice, the workflow was so far tested with snakemake version 7.32.4.\n\nmamba create --name snakemake -c conda-forge -c bioconda snakemake=7.32.4 python=3.11.6\n\n\n\n\nNanoITS can be installed via git:\n\ngit clone https://github.com/ndombrowski/NanoITS.git\n\nIf you do not have git installed, you can also download the folder from github. Therefore:\n\nGo to https://github.com/ndombrowski/NanoITS\nClick the green code button\nDownload zip\nExtract zip",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "source/prerequisites.html#prerequisites",
    "href": "source/prerequisites.html#prerequisites",
    "title": "Getting started",
    "section": "",
    "text": "To run the workflow you need to have mamba (conda also works) and snakemake installed.\nAll other software required by NanoITs will be installed by snakemake for you in a conda environment.\n\n\nInformation about installing mamba can be found here and for conda can be found here.\n\n\n\nAfter having installed conda/mamba you can install snakemake as follows. Notice, the workflow was so far tested with snakemake version 7.32.4.\n\nmamba create --name snakemake -c conda-forge -c bioconda snakemake=7.32.4 python=3.11.6\n\n\n\n\nNanoITS can be installed via git:\n\ngit clone https://github.com/ndombrowski/NanoITS.git\n\nIf you do not have git installed, you can also download the folder from github. Therefore:\n\nGo to https://github.com/ndombrowski/NanoITS\nClick the green code button\nDownload zip\nExtract zip",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "source/run_nanoITS.html#setup",
    "href": "source/run_nanoITS.html#setup",
    "title": "Run NanoITS",
    "section": "Setup",
    "text": "Setup\nTo run NanoITS you need to provide some information about the samples you want to analyse via the config.yaml file.\n\nModify the configuration file\nWhen using Snakemake a configuration file, also called config.yaml, allows you to modify some initial settings for your analyses.\nTo change the default settings and tell Snakemake where your data is located copy the config/config.yaml file found in the NanoITs folder to the folder in which you want to analyse your data, i.e. like this:\n\ncp &lt;path_to NanoITS_folder&gt;/config/config.yaml .\n\nYou can of course run your analyses in NanoITS folder you downloaded, but often its easier to separate tools from analyses.\nNext, open the config.yaml with an editor, such as nano. There are several things you can modify:\nThe project name\nYou can provide the project name in project: \"run_v1\". Your results will be generated in the folder you start the snakemake workflow in and the results will be generated in results/&lt;project_name&gt;. Your project name can contain letters, numbers _ and -. Please do not use other symbols, such as spaces or dots.\nThe mapping file\nHere, you need to provide a comma-separated mapping file that describes the samples you want to analyse, i.e. samples_file: \"input/mapping.csv\". This file needs to contain the following columns:\n\nsample: The names of your sample. This id will be used to label all files created in subsequent steps. In your sample names you can use letters, numbers and -. Please do not use other symbols spaces, dots or underscores in your sample names.\nbarcode: The barcode ID. Can be empty as it is not actively used in the workflow as of now\npath: Path to the fastq.gz files. The workflow accepts one file per barcode, so if you have more than one file merge this first using cat. Here, you can use a relative path (i.e. relative to the working directory you start the snakemake workflow in) or absolute.\n\nExample mapping file:\nsample,barcode,path\nbc01,barcode01,/path/barcode01.fastq.gz\nbc02,barcode02,/path/barcode02.fastq.gz\n...\nThe classifiers to use\nYou can choose what classifiers you want to use in classifiers: [\"minimap2\", \"kraken2\"]. Currently, two classifiers are implemented: (a) the alignment-based classifier minimap2 and (b) the kmer-based classifier kraken2. You can use both or either of the two classifiers.\nThe markers to investigate\nYou can select what markers you want to analyse in markers: [\"SSU\", \"ITS1\"]. The workflow was developed for primers targetting both the SSU and ITS1 but the workflow will also run for either option selected and we plan to in the future extend the workflow to also accept the LSU rRNA gene.\nOther parameters\nFinally, you can change tool specific parameters: If desired, there are several parameters that can be changed by the users, such as the numbers of threads to use, the settings for the read filtering or the classification and so on. The configuration file provides more information on each parameter.\nSince several steps of this workflow are quite resource intensive, so we recommend running this worklow on an HPC and set the numbers of threads used by each process accordingly."
  },
  {
    "objectID": "source/run_nanoITS.html#modify-the-configuration-file",
    "href": "source/run_nanoITS.html#modify-the-configuration-file",
    "title": "Run NanoITS",
    "section": "Modify the configuration file",
    "text": "Modify the configuration file\nTo run NanoITS you need to provide some information in the configuration file. In the NanoITs configuration file, also called config.yaml, you can provide the sample name, sample path and modify some parameters for the different software used.\nTo change the default settings and tell Snakemake where your data is located copy the config/config.yaml file found in the NanoITs folder to the folder in which you want to analyse your data, i.e. like this:\n\ncp &lt;path_to NanoITS_folder&gt;/config/config.yaml .\n\nYou can of course run your analyses in NanoITS folder you downloaded, but often its easier to separate software from analyses.\nNext, open the config.yaml with an editor, such as nano. There are several things you can modify:\nThe project name\nYou can provide the project name in project: \"run_v1\". Your results will be generated in the folder you start the snakemake workflow in and the results will be generated in results/&lt;project_name&gt; (results/run_v1 if you use the default settings). Your project name can contain letters, numbers, _ and -. Do not use other symbols, such as spaces or dots.\nThe mapping file\nHere, you need to provide the path to a comma-separated mapping file that describes the samples you want to analyse, i.e. samples_file: \"input/mapping.csv\". The mapping file itself needs to contain the following columns:\n\nsample: The names of your sample. This id will be used to label all files created in subsequent steps. Your sample names should be unique and only contain letters, numbers and -. Do not use other symbols, such as spaces, dots or underscores in your sample names.\nbarcode: The barcode ID. Can be empty as it is not actively used in the workflow as of now\npath: Path to the fastq.gz files. You can provide the relative path (i.e. relative to the working directory you start the snakemake workflow in) or absolute path (i.e. the location of a file or directory from the root directory(/)). The workflow accepts one file per barcode, so if you have more than one file merge these files first, for example using the cat command.\n\nExample mapping file:\nsample,barcode,path\nbc01,barcode01,/path/barcode01.fastq.gz\nbc02,barcode02,/path/barcode02.fastq.gz\n...\nIf you use the example, ensure that the mapping.csv file resides in a folder called input or change the config.yaml accordingly.\nThe classifiers to use\nYou can choose what classifiers you want to use in classifiers: [\"minimap2\", \"kraken2\"]. Currently, two classifiers are implemented: (a) the alignment-based classifier minimap2 and (b) the kmer-based classifier kraken2. You can use both or either of the two classifiers.\nThe markers to investigate\nYou can select what markers you want to analyse in markers: [\"SSU\", \"ITS1\", \"ITS2\"]. The workflow was developed for primers targeting both the SSU and ITS1/ITS2 but the workflow will also run for either option selected and we plan to in the future extend the workflow to also accept the LSU rRNA gene.\nOther parameters\nFinally, you can change tool specific parameters: If desired, there are several parameters that can be changed by the user, such as the numbers of threads to use, the settings for the read filtering or the classification and so on. The configuration file provides more information on each parameter.\nThe most important parameters to check out are the settings for the filtering of reads and the minimum length cutoff (min_its_length) for the SSU and ITS1/ITS2 sequences.\nSince several steps of this workflow are quite resource intensive, we recommend running this workflow on an HPC and set the numbers of threads accordingly.",
    "crumbs": [
      "Run NanoITS"
    ]
  },
  {
    "objectID": "source/references.html",
    "href": "source/references.html",
    "title": "References",
    "section": "",
    "text": "This workflow makes use of tools developed by many different people, so please cite these tools accordingly. To make this easier you find here a list of tool requirements and (if available) the associated publications.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "source/run_nanoITS.html#generate-a-report",
    "href": "source/run_nanoITS.html#generate-a-report",
    "title": "Run NanoITS",
    "section": "Generate a report",
    "text": "Generate a report\nAfter a successful run, you can create a report with some of the key output files as follows:\n\nsnakemake --report report.html \\\n  --configfile config/config.yaml \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile\n\nAt the moment the report contains:\n\nA schematic of the steps executed\nInformation about the reads before and after quality filtering (one file for each sample)\nInformation about the number of SSU and ITS1 sequences extracted before and after quality filtering\nThe results from the different classifiers represented as barplots (total counts and relative abundance) for different taxonomic ranks\nStatistics of the snakemake run\n\nAdditionally, you can find some OTU tables in these locations:\n\nresults/&lt;project_name&gt;/tables/{marker}_otu_table.txt: An OTU table based on the taxonomy assignment and including the counts for each sample and classifier. The table is generated once for each marker generated.\nresults/&lt;project_name&gt;/tables/{marker}_otu_table_filtered..txt: A filtered OTU table based on the taxonomy assignment and including the counts for each sample and classifier. The table is generated once for each marker generated. The filtering discards samples with =&lt;20 reads and singletons and the barplots in the reports where generated from the filtered OTU table.\nresults/&lt;project_name&gt;/classification/{classifier}/{marker}.merged.outmat.tsv: An OTU table based on the taxonomy assignment and including the counts for each sample. One separate table is generated for each marker investigated and classifier used.\n\n\nGenerate a report with the old report format\nSnakemake v7.32.4 generates reports in a new format, which is a bit more convoluted than the older format. If you prefer the old format, you can create if running snakemake –report with an older version, such as snakemake v6.8.0.",
    "crumbs": [
      "Run NanoITS"
    ]
  },
  {
    "objectID": "source/output.html",
    "href": "source/output.html",
    "title": "Generated files",
    "section": "",
    "text": "Other than the Snakemake report, this workflow keeps most intermediate files to allow the user to inspect any output as needed. More specifically, the results/{project} folder contains the following:\n├── benchmarks\n├── classification\n│    └── {classifier}\n│        └── {sample}\n│           ├── {sample}_{marker}_kraken2.otumat\n│           ├── {sample}_{marker}_kraken2.out\n│           ├── {sample}_{marker}_kraken2.report\n│           ├── {sample}_{marker}_kraken2.taxlist\n│           └── {sample}_{marker}_kraken2.taxmat\n├── itsx\n│   ├── avg_seq_length.txt\n│   ├── avg_seq_nr.txt\n│   └── {sample}\n│           ├── {sample}.graph\n│           ├── {sample}.hmmer.table\n│           ├── {sample}.{marker}.fasta\n│           ├── {sample}_{marker}_final.fasta\n│           ├── {sample}_no_detections.fasta\n│           ├── {sample}_no_detections.txt\n│           ├── {sample}.positions.txt\n│           ├── {sample}.problematic.txt\n│           └── {sample}.summary.txt\n├── plotting\n│   └── {marker}\n│    │   └── {marker}_Barplot_{ra,counts}_{taxonomy}_rank.pdf\n├── quality_checks\n│    ├── 1_unfiltered\n│    │   ├── nanostat\n│    │   │   └── {sample}_unfiltered_stats.txt\n│    │   └── pistis\n│    |       └── {sample}.pdf\n│    └── 2_filtered\n│        ├── nanostat\n│        │   └── {sample}_filtered_stats.txt\n│        └── pistis\n│            └── {sample}.pdf\n├── reads\n│    └── 2_qual_filtered_reads\n│        ├── 1_porechop\n│        |    └── {sample}.fastqc.gz\n│        ├── 2_chopper\n│        |    └── {sample}.fastqc.gz\n│        └── 3_fasta\n│            └── {sample}.fasta\n└── tables\n    ├── {marker}_otu_table_filtered.txt\n    └── {marker}_otu_table.txt",
    "crumbs": [
      "Generated files"
    ]
  },
  {
    "objectID": "source/references.html#used-databases",
    "href": "source/references.html#used-databases",
    "title": "References",
    "section": "Used databases",
    "text": "Used databases\n\nSilva fasta sequences: SILVA_138.1_SSURef_NR99 downloaded from here (Quast et al. 2013)\nSilva link to ncbi taxonomy: taxmap_embl-ebi_ena_ssu_ref_nr99_138.1.txt downloaded from here\nUnite fasta sequences: sh_general_release_all_18.07.2023 (Nilsson et al. 2019)\nNCBI taxonomy dump downloaded on 19102023",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "source/references.html#used-software",
    "href": "source/references.html#used-software",
    "title": "References",
    "section": "Used software",
    "text": "Used software\nThe version numbers are the exact versions used to develop this workflow.\n\nSnakemake v7.32.4 (Mölder et al. 2021)\nPython v3.6.15\n\nbiopython v1.79\ntabulate v0.8.10\npandas v1.1.5\nmatplotlib v3.3.4\nseaborn 0.11.2\n\nR v4.2\n\ntidyverse v1.3.2 (Wickham et al. 2019)\nphyloseq v1.42.0 (McMurdie and Holmes 2013)\nmicrobiome v1.20.0 (Lahti and Shetty 2012)\n\nNanoPack v1.1.0 (De Coster and Rademakers 2023), which comes with the following tools used in this workflow:\n\nNanoStat v1.6.0\nChopper v0.6.0\n\nPistis v0.3.3 github_link\nPorechop v0.2.4 github_link\nITSx v1.1.3 (Bengtsson-Palme et al. 2013)\nMinimap2 v2.24 (Li 2018)\nKraken2 v2.1.3 (Wood, Lu, and Langmead 2019)",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "source/references.html#reference-list",
    "href": "source/references.html#reference-list",
    "title": "References",
    "section": "Reference list",
    "text": "Reference list\n\n\nBengtsson-Palme, Johan, Martin Ryberg, Martin Hartmann, Sara Branco, Zheng Wang, Anna Godhe, Pierre De Wit, et al. 2013. “Improved Software Detection and Extraction of ITS1 and ITS2 from Ribosomal ITS Sequences of Fungi and Other Eukaryotes for Analysis of Environmental Sequencing Data.” Methods in Ecology and Evolution 4 (10): 914–19. https://doi.org/10.1111/2041-210X.12073.\n\n\nDe Coster, Wouter, and Rosa Rademakers. 2023. “NanoPack2: Population-Scale Evaluation of Long-Read Sequencing Data.” Bioinformatics 39 (5): btad311. https://doi.org/10.1093/bioinformatics/btad311.\n\n\nLahti, Leo, and Sudarshan Shetty. 2012. “Microbiome r Package.”\n\n\nLi, Heng. 2018. “Minimap2: Pairwise Alignment for Nucleotide Sequences.” Bioinformatics 34 (18): 3094–3100. https://doi.org/10.1093/bioinformatics/bty191.\n\n\nMcMurdie, Paul J., and Susan Holmes. 2013. “Phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data.” PLOS ONE 8 (4): e61217. https://doi.org/10.1371/journal.pone.0061217.\n\n\nMölder, Felix, Kim Philipp Jablonski, Brice Letcher, Michael B. Hall, Christopher H. Tomkins-Tinch, Vanessa Sochat, Jan Forster, et al. 2021. “Sustainable Data Analysis with Snakemake.” F1000Research 10 (April): 33. https://doi.org/10.12688/f1000research.29032.2.\n\n\nNilsson, Rolf Henrik, Karl-Henrik Larsson, Andy F S Taylor, Johan Bengtsson-Palme, Thomas S Jeppesen, Dmitry Schigel, Peter Kennedy, et al. 2019. “The UNITE Database for Molecular Identification of Fungi: Handling Dark Taxa and Parallel Taxonomic Classifications.” Nucleic Acids Research 47 (D1): D259–64. https://doi.org/10.1093/nar/gky1022.\n\n\nQuast, Christian, Elmar Pruesse, Pelin Yilmaz, Jan Gerken, Timmy Schweer, Pablo Yarza, Jörg Peplies, and Frank Oliver Glöckner. 2013. “The SILVA Ribosomal RNA Gene Database Project: Improved Data Processing and Web-Based Tools.” Nucleic Acids Research 41 (D1): D590–96. https://doi.org/10.1093/nar/gks1219.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWood, Derrick E., Jennifer Lu, and Ben Langmead. 2019. “Improved Metagenomic Analysis with Kraken 2.” Genome Biology 20 (1): 257. https://doi.org/10.1186/s13059-019-1891-0.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "NanoITS",
    "section": "Quick start",
    "text": "Quick start\nTo run NanoITs, install conda and use it to installed snakemake as follows:\n\nmamba create --name snakemake -c conda-forge -c bioconda snakemake=7.32.4 python=3.11.6\n\nAfterwards, you can clone the directory from github via:\n\ngit clone https://github.com/ndombrowski/NanoITS.git\n\nProvide your sample names and path to the samples as a comma-separated file. To view an example have a look at example_files/mapping.csv. The sample names should be unique and consist of letters, numbers and - only. The barcode column can be left empty as it is not yet implemented. The path should contain the path to your demultiplexed, compressed fastq file(s).\nAdjust config/config.yaml to configure the location of your mapping file as well as specify the parameters used by NanoITs.\nNanoITs can then be run with (adjust the cores based on your system):\n\n#perform a dry-run to ensure everything is working correctly\nsnakemake --use-conda --cores &lt;nr_cores&gt; \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile \\\n  --configfile config/config.yaml \\\n  --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  \\\n  -np \n\n#run NanoITS\nsnakemake --use-conda --cores &lt;nr_cores&gt; \\\n  -s &lt;path_to_NanoITS_install&gt;/workflow/Snakefile \\\n  --configfile config/config.yaml \\\n  --conda-prefix &lt;path_to_NanoITS_install&gt;/workflow/.snakemake/conda  \\\n  --rerun-incomplete --nolock",
    "crumbs": [
      "NanoITS"
    ]
  },
  {
    "objectID": "source/dev_notes.html",
    "href": "source/dev_notes.html",
    "title": "General notes",
    "section": "",
    "text": "git checkout -b its2-dev\nWork on integrating ITS2 into\n\nconfig\nkraken2.smk\nminimap2.smk\n\ngit checkout main\ngit pull origin main\ngit merge its2-dev"
  },
  {
    "objectID": "source/dev_notes.html#implement-its2",
    "href": "source/dev_notes.html#implement-its2",
    "title": "General notes",
    "section": "",
    "text": "git checkout -b its2-dev\nWork on integrating ITS2 into\n\nconfig\nkraken2.smk\nminimap2.smk\n\ngit checkout main\ngit pull origin main\ngit merge its2-dev"
  },
  {
    "objectID": "source/dev_notes.html#update",
    "href": "source/dev_notes.html#update",
    "title": "General notes",
    "section": "Update",
    "text": "Update"
  }
]